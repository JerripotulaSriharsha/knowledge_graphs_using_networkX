{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ad19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Narwal\\knowledge_graphs_using_networkX\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    # last resort\n",
    "    with open(path, \"r\", errors=\"replace\") as f:\n",
    "        return f.read()\n",
    "\n",
    "content = read_text(\"ft_guide.txt\")\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1200, chunk_overlap=100)\n",
    "texts = text_splitter.split_text(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df890d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d30f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11f1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "-Goal-\n",
    "Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: One of the following types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\"{{tuple_delimiter}}<entity_name>{{tuple_delimiter}}<entity_type>{{tuple_delimiter}}<entity_description>)\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n",
    "Format each relationship as (\"relationship\"{{tuple_delimiter}}<source_entity>{{tuple_delimiter}}<target_entity>{{tuple_delimiter}}<relationship_description>{{tuple_delimiter}}<relationship_strength>)\n",
    "\n",
    "3. Return output in The primary language of the provided text is \"English.\" as a single list of all the entities and relationships identified in steps 1 and 2. Use **{{record_delimiter}}** as the list delimiter.\n",
    "\n",
    "4. If you have to translate into The primary language of the provided text is \"English.\", just translate the descriptions, nothing else!\n",
    "\n",
    "5. When finished, output {{completion_delimiter}}.\n",
    "\n",
    "-Examples-\n",
    "######################\n",
    "\n",
    "Example 1:\n",
    "\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text:\n",
    " LLMs to create synthetic samples that mimic clients’ private data distribution using\n",
    "differential privacy. This approach significantly boosts SLMs’ performance by approximately 5% while\n",
    "maintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\n",
    "solely on local private data.\n",
    "In healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\n",
    "without transferring sensitive information. This approach ensures data privacy while enabling the de-\n",
    "velopment of robust, generalisable AI systems.\n",
    "8https://ai.meta.com/responsible-ai/\n",
    "9https://huggingface.co/docs/hub/en/model-cards\n",
    "10https://www.tensorflow.org/responsible_ai/privacy/guide\n",
    "101 Frameworks for Enhancing Security\n",
    "Adversarial training and robust security measures[111] are essential for protecting fine-tuned models\n",
    "against attacks. The adversarial training approach involves training models with adversarial examples\n",
    "to improve their resilience against malicious inputs. Microsoft Azure’s\n",
    "------------------------\n",
    "output:\n",
    "(\"entity\"{{tuple_delimiter}}DIFFERENTIAL PRIVACY{{tuple_delimiter}}differential privacy{{tuple_delimiter}}Differential privacy is a technique used to create synthetic samples that mimic clients' private data distribution while maintaining data privacy with a minimal privacy budget{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}HEALTHCARE{{tuple_delimiter}}healthcare{{tuple_delimiter}}In healthcare, federated fine-tuning allows hospitals to collaboratively train models on patient data without transferring sensitive information, ensuring data privacy{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}federated learning{{tuple_delimiter}}Federated learning is a method that enables collaborative model training on decentralized data sources, such as hospitals, without sharing sensitive information{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}ADVERSARIAL TRAINING{{tuple_delimiter}}adversarial training{{tuple_delimiter}}Adversarial training involves training models with adversarial examples to improve their resilience against malicious inputs{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}SECURITY MEASURES{{tuple_delimiter}}security measures{{tuple_delimiter}}Robust security measures are essential for protecting fine-tuned models against attacks{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}DIFFERENTIAL PRIVACY{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}Differential privacy is used in federated learning to maintain data privacy while training models collaboratively{{tuple_delimiter}}8{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}HEALTHCARE{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}Federated learning is applied in healthcare to train models on patient data without transferring sensitive information{{tuple_delimiter}}9{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ADVERSARIAL TRAINING{{tuple_delimiter}}SECURITY MEASURES{{tuple_delimiter}}Adversarial training is a security measure used to protect models against attacks by improving their resilience{{tuple_delimiter}}8{{completion_delimiter}}\n",
    "#############################\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text:\n",
    "ARD [82] is an innovative open-source tool developed to enhance the safety of interactions\n",
    "with large language models (LLMs). This tool addresses three critical moderation tasks: detecting\n",
    "2https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\n",
    "63 harmful intent in user prompts, identifying safety risks in model responses, and determining when a\n",
    "model appropriately refuses unsafe requests. Central to its development is WILDGUARD MIX3, a\n",
    "meticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\n",
    "adversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\n",
    "for training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\n",
    "for evaluation.\n",
    "The WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\n",
    "TRAIN dataset, enabling it to perform all\n",
    "------------------------\n",
    "output:\n",
    "```plaintext\n",
    "(\"entity\"{{tuple_delimiter}}ARD{{tuple_delimiter}}open-source tool{{tuple_delimiter}}ARD is an innovative open-source tool developed to enhance the safety of interactions with large language models by addressing moderation tasks such as detecting harmful intent, identifying safety risks, and determining appropriate refusals of unsafe requests)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}large language model{{tuple_delimiter}}Large language models (LLMs) are advanced AI models designed to understand and generate human-like text, which ARD aims to interact with safely)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD MIX3 is a meticulously curated dataset comprising 92,000 labeled examples, including benign prompts and adversarial attempts, used for training and evaluating safety measures in language models)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD TRAIN is a subset of the WILDGUARD MIX3 dataset used specifically for training the model on safety measures)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD TEST{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD TEST is a subset of the WILDGUARD MIX3 dataset consisting of high-quality human-annotated examples used for evaluating the model's performance)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}MISTRAL-7B{{tuple_delimiter}}large language model{{tuple_delimiter}}Mistral-7B is a language model that the WILDGUARD model is fine-tuned on using the WILDGUARD TRAIN dataset to enhance its safety performance)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}ADVERSARIAL ATTEMPTS{{tuple_delimiter}}adversarial training{{tuple_delimiter}}Adversarial attempts are part of the WILDGUARD MIX3 dataset, used to test and improve the model's ability to handle unsafe or harmful inputs)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}SAFETY MEASURES{{tuple_delimiter}}security measures{{tuple_delimiter}}Safety measures are protocols and techniques implemented to ensure that large language models interact safely with users, which ARD and the WILDGUARD dataset aim to enhance)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ARD{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}ARD is designed to enhance the safety of interactions with large language models by addressing critical moderation tasks{{tuple_delimiter}}8)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ARD{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}ARD uses the WILDGUARD MIX3 dataset to train and evaluate its moderation capabilities{{tuple_delimiter}}7)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}WILDGUARD TRAIN is a subset of the WILDGUARD MIX3 dataset used for training{{tuple_delimiter}}9)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}WILDGUARD TEST{{tuple_delimiter}}WILDGUARD TEST is a subset of the WILDGUARD MIX3 dataset used for evaluation{{tuple_delimiter}}9)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}MISTRAL-7B{{tuple_delimiter}}The WILDGUARD TRAIN dataset is used to fine-tune the Mistral-7B language model{{tuple_delimiter}}8)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ADVERSARIAL ATTEMPTS{{tuple_delimiter}}SAFETY MEASURES{{tuple_delimiter}}Adversarial attempts are used to test and improve safety measures in language models{{tuple_delimiter}}7)\n",
    "{{completion_delimiter}}\n",
    "```\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text: {input_text}\n",
    "######################\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550f7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517af72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input_text\": texts[25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5df67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```plaintext\n",
      "(\"entity\"{tuple_delimiter}LORA{tuple_delimiter}adapter architecture{tuple_delimiter}LoRA is a fine-tuning technique that introduces two low-rank matrices to approximate weight updates, significantly reducing the number of trainable parameters and improving efficiency in memory and computation for large models{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}QLORA{tuple_delimiter}adapter architecture{tuple_delimiter}QLoRA is an extended version of LoRA that quantises weight parameters to 4-bit precision, allowing for greater memory efficiency and enabling fine-tuning on less powerful hardware while maintaining performance levels comparable to traditional methods{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}DORA{tuple_delimiter}adapter architecture{tuple_delimiter}Weight-Decomposed Low-Rank Adaptation (DoRA) is a fine-tuning methodology that decomposes model weights into magnitude and directional components, leveraging LoRA's efficiency for substantial updates without altering the entire model architecture{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}HUGGING FACE LIBRARY{tuple_delimiter}open-source tool{tuple_delimiter}The Hugging Face library is an open-source tool that provides templates and configurations for fine-tuning large language models, including support for LoRA and QLoRA methods{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}PRE-TRAINED MODELS{tuple_delimiter}large language model{tuple_delimiter}Pre-trained models are AI models that have been trained on large datasets and can be fine-tuned for specific tasks, such as natural language processing and vision-language applications{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}LORA{tuple_delimiter}QLORA{tuple_delimiter}QLoRA is an extension of LoRA that enhances its memory efficiency by quantising weight parameters, thus building upon the foundational principles of LoRA{tuple_delimiter}8{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}LORA{tuple_delimiter}DORA{tuple_delimiter}DoRA builds on the principles of LoRA by optimizing pre-trained models through weight decomposition, thus enhancing the adaptability and efficiency of the fine-tuning process{tuple_delimiter}9{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}HUGGING FACE LIBRARY{tuple_delimiter}LORA{tuple_delimiter}The Hugging Face library provides tools and templates for implementing the LoRA fine-tuning method, facilitating its application in model training{tuple_delimiter}8{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}HUGGING FACE LIBRARY{tuple_delimiter}QLORA{tuple_delimiter}The Hugging Face library supports the QLoRA method, providing configurations for quantisation and fine-tuning of large language models{tuple_delimiter}8{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}HUGGING FACE LIBRARY{tuple_delimiter}DORA{tuple_delimiter}The Hugging Face library includes the LoraConfig package, which is essential for incorporating DoRA into the fine-tuning process{tuple_delimiter}8{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}PRE-TRAINED MODELS{tuple_delimiter}LORA{tuple_delimiter}LoRA is specifically designed for fine-tuning pre-trained models, allowing for efficient updates while maintaining the integrity of the original model{tuple_delimiter}9{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}PRE-TRAINED MODELS{tuple_delimiter}DORA{tuple_delimiter}DoRA optimizes pre-trained models by leveraging the efficiency of LoRA, thus enhancing their performance across various tasks{tuple_delimiter}9{completion_delimiter}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48e68175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fa72024a-fa11-4c0c-9b32-070d074cde4d</td>\n",
       "      <td>0</td>\n",
       "      <td>VENKATESH BALAVADHANI PARTHASARATHY</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Venkatesh Balavadhani Parthasarathy is one of ...</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df0e3246-70fe-40e3-baca-8a3c01140011</td>\n",
       "      <td>1</td>\n",
       "      <td>AHTSHAM ZAFAR</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Ahtsham Zafar is an accomplished author recogn...</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dffb68e8-6683-4f2c-931c-5d2f14c09d11</td>\n",
       "      <td>2</td>\n",
       "      <td>AAFAQ KHAN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Aafaq Khan is one of the authors of the techni...</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0804f87b-0d37-47e1-9e6d-fe2224c55170</td>\n",
       "      <td>3</td>\n",
       "      <td>ARSALAN SHAHID</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Arsalan Shahid is one of the authors of the te...</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23a79c80-7299-4726-9b8f-db184bf58bee</td>\n",
       "      <td>4</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>CeADAR is Ireland’s Centre for AI, located at ...</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  fa72024a-fa11-4c0c-9b32-070d074cde4d                  0   \n",
       "1  df0e3246-70fe-40e3-baca-8a3c01140011                  1   \n",
       "2  dffb68e8-6683-4f2c-931c-5d2f14c09d11                  2   \n",
       "3  0804f87b-0d37-47e1-9e6d-fe2224c55170                  3   \n",
       "4  23a79c80-7299-4726-9b8f-db184bf58bee                  4   \n",
       "\n",
       "                                 title          type  \\\n",
       "0  VENKATESH BALAVADHANI PARTHASARATHY        PERSON   \n",
       "1                        AHTSHAM ZAFAR        PERSON   \n",
       "2                           AAFAQ KHAN        PERSON   \n",
       "3                       ARSALAN SHAHID        PERSON   \n",
       "4                               CEADAR  ORGANIZATION   \n",
       "\n",
       "                                         description  \\\n",
       "0  Venkatesh Balavadhani Parthasarathy is one of ...   \n",
       "1  Ahtsham Zafar is an accomplished author recogn...   \n",
       "2  Aafaq Khan is one of the authors of the techni...   \n",
       "3  Arsalan Shahid is one of the authors of the te...   \n",
       "4  CeADAR is Ireland’s Centre for AI, located at ...   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "1  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "2  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "3  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "4  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entities_raw = r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\\output\\entities.parquet\"\n",
    "entities_raw = pd.read_parquet(entities_raw)  \n",
    "entities = entities_raw[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"human_readable_id\",\n",
    "        \"title\",\n",
    "        \"type\",\n",
    "        \"description\",\n",
    "        \"text_unit_ids\",\n",
    "    ]\n",
    "].copy()\n",
    "entities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d27de588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1a7787b6-777f-48b7-934c-7bfc7d70ce62</td>\n",
       "      <td>0</td>\n",
       "      <td>VENKATESH BALAVADHANI PARTHASARATHY</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Venkatesh Balavadhani Parthasarathy is affilia...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b5d550f-a809-4e79-955a-c36ca0848a20</td>\n",
       "      <td>1</td>\n",
       "      <td>AHTSHAM ZAFAR</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Ahtsham Zafar is affiliated with CeADAR as an ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0cade9e1-afd7-480d-a2d7-3b771362d339</td>\n",
       "      <td>2</td>\n",
       "      <td>AAFAQ KHAN</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Aafaq Khan is affiliated with CeADAR as an aut...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e5738970-ce0e-4db8-9b6c-8a00745bd71d</td>\n",
       "      <td>3</td>\n",
       "      <td>ARSALAN SHAHID</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Arsalan Shahid is affiliated with CeADAR as an...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188cd76f-4b80-42bd-8aa4-5dfb3a768d86</td>\n",
       "      <td>4</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>UNIVERSITY COLLEGE DUBLIN</td>\n",
       "      <td>CeADAR is a research group within University C...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>[30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  1a7787b6-777f-48b7-934c-7bfc7d70ce62                  0   \n",
       "1  4b5d550f-a809-4e79-955a-c36ca0848a20                  1   \n",
       "2  0cade9e1-afd7-480d-a2d7-3b771362d339                  2   \n",
       "3  e5738970-ce0e-4db8-9b6c-8a00745bd71d                  3   \n",
       "4  188cd76f-4b80-42bd-8aa4-5dfb3a768d86                  4   \n",
       "\n",
       "                                source                     target  \\\n",
       "0  VENKATESH BALAVADHANI PARTHASARATHY                     CEADAR   \n",
       "1                        AHTSHAM ZAFAR                     CEADAR   \n",
       "2                           AAFAQ KHAN                     CEADAR   \n",
       "3                       ARSALAN SHAHID                     CEADAR   \n",
       "4                               CEADAR  UNIVERSITY COLLEGE DUBLIN   \n",
       "\n",
       "                                         description  weight  combined_degree  \\\n",
       "0  Venkatesh Balavadhani Parthasarathy is affilia...     8.0                7   \n",
       "1  Ahtsham Zafar is affiliated with CeADAR as an ...     8.0                8   \n",
       "2  Aafaq Khan is affiliated with CeADAR as an aut...     8.0                7   \n",
       "3  Arsalan Shahid is affiliated with CeADAR as an...     8.0                7   \n",
       "4  CeADAR is a research group within University C...     9.0                8   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "1  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "2  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "3  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  \n",
       "4  [30c05f4301f700b0f85dd15af3f3ba0f50fdd82b8cefb...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\\output\\relationships.parquet\"\n",
    "relationships = pd.read_parquet(path)\n",
    "relationships = relationships[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"human_readable_id\",\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"description\",\n",
    "        \"weight\",\n",
    "        \"combined_degree\",\n",
    "        \"text_unit_ids\",\n",
    "    ]\n",
    "].copy()\n",
    "relationships.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5e1639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>degree</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43264a05-af44-47d5-8707-63c53792334e</td>\n",
       "      <td>807</td>\n",
       "      <td>ADAM LERER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aebd3291-ad0e-49f3-8687-bddcba7756e0</td>\n",
       "      <td>763</td>\n",
       "      <td>ARXIV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91fdee27-db4c-442c-87e4-add5d87a4bfb</td>\n",
       "      <td>804</td>\n",
       "      <td>ADAM PASZKE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316b2889-c1e9-405f-aaf2-87f938641440</td>\n",
       "      <td>757</td>\n",
       "      <td>ALEXANDER RATNER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7df79d37-fb66-432c-bdf1-ff3b5a6c388f</td>\n",
       "      <td>765</td>\n",
       "      <td>ANA MARASOVIĆ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>3be4137f-98b7-43d4-8292-d8a7bce3b86d</td>\n",
       "      <td>533</td>\n",
       "      <td>DATA CURATION</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>8cef081b-2ee5-4bce-841b-f39d16e425b3</td>\n",
       "      <td>499</td>\n",
       "      <td>DISTRIBUTED TRAINING</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>80b05f67-9c3f-4749-83c0-ced4b6eaf748</td>\n",
       "      <td>535</td>\n",
       "      <td>INFERENCE</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>e35449d5-eae0-46f8-b62d-7a585cd16df0</td>\n",
       "      <td>534</td>\n",
       "      <td>REINFORCEMENT LEARNING WITH HUMAN FEEDBACK</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>9a9169ee-6a09-4797-a0eb-45451ffda948</td>\n",
       "      <td>527</td>\n",
       "      <td>RIVA NMT</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  human_readable_id  \\\n",
       "0     43264a05-af44-47d5-8707-63c53792334e                807   \n",
       "1     aebd3291-ad0e-49f3-8687-bddcba7756e0                763   \n",
       "2     91fdee27-db4c-442c-87e4-add5d87a4bfb                804   \n",
       "3     316b2889-c1e9-405f-aaf2-87f938641440                757   \n",
       "4     7df79d37-fb66-432c-bdf1-ff3b5a6c388f                765   \n",
       "...                                    ...                ...   \n",
       "1050  3be4137f-98b7-43d4-8292-d8a7bce3b86d                533   \n",
       "1051  8cef081b-2ee5-4bce-841b-f39d16e425b3                499   \n",
       "1052  80b05f67-9c3f-4749-83c0-ced4b6eaf748                535   \n",
       "1053  e35449d5-eae0-46f8-b62d-7a585cd16df0                534   \n",
       "1054  9a9169ee-6a09-4797-a0eb-45451ffda948                527   \n",
       "\n",
       "                                           title  community  level  degree  \\\n",
       "0                                     ADAM LERER          0      0       1   \n",
       "1                                          ARXIV          0      0      65   \n",
       "2                                    ADAM PASZKE          0      0       1   \n",
       "3                               ALEXANDER RATNER          0      0       1   \n",
       "4                                 ANA MARASOVIĆ          0      0       1   \n",
       "...                                          ...        ...    ...     ...   \n",
       "1050                               DATA CURATION         89      2       1   \n",
       "1051                        DISTRIBUTED TRAINING         89      2       1   \n",
       "1052                                   INFERENCE         89      2       1   \n",
       "1053  REINFORCEMENT LEARNING WITH HUMAN FEEDBACK         89      2       1   \n",
       "1054                                    RIVA NMT         89      2       1   \n",
       "\n",
       "        x    y  \n",
       "0     0.0  0.0  \n",
       "1     0.0  0.0  \n",
       "2     0.0  0.0  \n",
       "3     0.0  0.0  \n",
       "4     0.0  0.0  \n",
       "...   ...  ...  \n",
       "1050  0.0  0.0  \n",
       "1051  0.0  0.0  \n",
       "1052  0.0  0.0  \n",
       "1053  0.0  0.0  \n",
       "1054  0.0  0.0  \n",
       "\n",
       "[1055 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entities_raw = pd.read_parquet(r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\\output\\entities.parquet\")\n",
    "communities = pd.read_parquet(r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\\output\\communities.parquet\")\n",
    "\n",
    "nodes = (\n",
    "    communities[[\"community\", \"level\", \"entity_ids\"]]\n",
    "    .explode(\"entity_ids\")\n",
    "    .rename(columns={\"entity_ids\": \"id\"})\n",
    "    .merge(\n",
    "        entities_raw[[\"id\", \"human_readable_id\", \"title\", \"degree\", \"x\", \"y\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    [[\"id\", \"human_readable_id\", \"title\", \"community\", \"level\", \"degree\", \"x\", \"y\"]]\n",
    ")\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "063a6377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>parent</th>\n",
       "      <th>children</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>full_content</th>\n",
       "      <th>rank</th>\n",
       "      <th>rating_explanation</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>period</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4d06c18b850a48cea2440cd242f09886</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fine-Tuning in Machine Learning</td>\n",
       "      <td>The community focuses on the process of fine-t...</td>\n",
       "      <td># Fine-Tuning in Machine Learning\\n\\nThe commu...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'Fine-tuning is a vital proce...</td>\n",
       "      <td>{\\n    \"title\": \"Fine-Tuning in Machine Learni...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b0df43931ee4b958f3037bb17e34c23</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>[]</td>\n",
       "      <td>Pre-Training and Comparative Overview of LLMs</td>\n",
       "      <td>The community focuses on the pre-training phas...</td>\n",
       "      <td># Pre-Training and Comparative Overview of LLM...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The impact severity rating is moderate due to ...</td>\n",
       "      <td>[{'explanation': 'The pre-training phase is a ...</td>\n",
       "      <td>{\\n    \"title\": \"Pre-Training and Comparative ...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de9ed0f44c0a496b9a739efbb3073a22</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>[]</td>\n",
       "      <td>NVIDIA and AI Technologies Community</td>\n",
       "      <td>The community centers around NVIDIA, a leading...</td>\n",
       "      <td># NVIDIA and AI Technologies Community\\n\\nThe ...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to NVID...</td>\n",
       "      <td>[{'explanation': 'NVIDIA is a prominent player...</td>\n",
       "      <td>{\\n    \"title\": \"NVIDIA and AI Technologies Co...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d308efe6878b490ea1815ecb3756e10c</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>[]</td>\n",
       "      <td>NVIDIA NeMo and AI Model Development</td>\n",
       "      <td>The community centers around NVIDIA NeMo, a fr...</td>\n",
       "      <td># NVIDIA NeMo and AI Model Development\\n\\nThe ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'NVIDIA NeMo serves as the fo...</td>\n",
       "      <td>{\\n    \"title\": \"NVIDIA NeMo and AI Model Deve...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19fb04ead5044b6aaec4f9e76a4d4468</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv Research Community</td>\n",
       "      <td>The arXiv research community is centered aroun...</td>\n",
       "      <td># arXiv Research Community\\n\\nThe arXiv resear...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'arXiv is a leading repositor...</td>\n",
       "      <td>{\\n    \"title\": \"arXiv Research Community\",\\n ...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  human_readable_id  community  level  \\\n",
       "0  4d06c18b850a48cea2440cd242f09886                 86         86      2   \n",
       "1  4b0df43931ee4b958f3037bb17e34c23                 87         87      2   \n",
       "2  de9ed0f44c0a496b9a739efbb3073a22                 88         88      2   \n",
       "3  d308efe6878b490ea1815ecb3756e10c                 89         89      2   \n",
       "4  19fb04ead5044b6aaec4f9e76a4d4468                 21         21      1   \n",
       "\n",
       "   parent children                                          title  \\\n",
       "0      24       []                Fine-Tuning in Machine Learning   \n",
       "1      24       []  Pre-Training and Comparative Overview of LLMs   \n",
       "2      30       []           NVIDIA and AI Technologies Community   \n",
       "3      30       []           NVIDIA NeMo and AI Model Development   \n",
       "4       0       []                       arXiv Research Community   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The community focuses on the process of fine-t...   \n",
       "1  The community focuses on the pre-training phas...   \n",
       "2  The community centers around NVIDIA, a leading...   \n",
       "3  The community centers around NVIDIA NeMo, a fr...   \n",
       "4  The arXiv research community is centered aroun...   \n",
       "\n",
       "                                        full_content  rank  \\\n",
       "0  # Fine-Tuning in Machine Learning\\n\\nThe commu...   8.0   \n",
       "1  # Pre-Training and Comparative Overview of LLM...   4.0   \n",
       "2  # NVIDIA and AI Technologies Community\\n\\nThe ...   8.5   \n",
       "3  # NVIDIA NeMo and AI Model Development\\n\\nThe ...   8.0   \n",
       "4  # arXiv Research Community\\n\\nThe arXiv resear...   8.5   \n",
       "\n",
       "                                  rating_explanation  \\\n",
       "0  The impact severity rating is high due to the ...   \n",
       "1  The impact severity rating is moderate due to ...   \n",
       "2  The impact severity rating is high due to NVID...   \n",
       "3  The impact severity rating is high due to the ...   \n",
       "4  The impact severity rating is high due to the ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'Fine-tuning is a vital proce...   \n",
       "1  [{'explanation': 'The pre-training phase is a ...   \n",
       "2  [{'explanation': 'NVIDIA is a prominent player...   \n",
       "3  [{'explanation': 'NVIDIA NeMo serves as the fo...   \n",
       "4  [{'explanation': 'arXiv is a leading repositor...   \n",
       "\n",
       "                                   full_content_json      period  size  \n",
       "0  {\\n    \"title\": \"Fine-Tuning in Machine Learni...  2025-12-31    11  \n",
       "1  {\\n    \"title\": \"Pre-Training and Comparative ...  2025-12-31     2  \n",
       "2  {\\n    \"title\": \"NVIDIA and AI Technologies Co...  2025-12-31     8  \n",
       "3  {\\n    \"title\": \"NVIDIA NeMo and AI Model Deve...  2025-12-31     6  \n",
       "4  {\\n    \"title\": \"arXiv Research Community\",\\n ...  2025-12-31    57  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_reports = pd.read_parquet(\n",
    "    r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\\output\\community_reports.parquet\"\n",
    ")\n",
    "\n",
    "community_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a305e0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'human_readable_id', 'title', 'type', 'description',\n",
       "       'text_unit_ids'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'human_readable_id', 'source', 'target', 'description', 'weight',\n",
       "       'combined_degree', 'text_unit_ids'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'human_readable_id', 'community', 'level', 'parent', 'children',\n",
       "       'title', 'summary', 'full_content', 'rank', 'rating_explanation',\n",
       "       'findings', 'full_content_json', 'period', 'size'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'human_readable_id', 'title', 'community', 'level', 'degree', 'x',\n",
       "       'y'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(entities.columns,relationships.columns,community_reports.columns,nodes.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3ab8368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['99aeb224-a1cc-45b6-80a4-a2a33a6ee8ee', 900, 'KEVIN CLARK',\n",
       "        'PERSON',\n",
       "        'Kevin Clark is a researcher focused on large-scale machine learning.',\n",
       "        array(['582cc63e645b76407bfddcc88baf3064807b8d6557ddf9ce2b5dc1f8f0bd0977986f4875d2309c3648ec4b1a283a7fb8924cd32e0918ebde112169d28bbd71c0'],\n",
       "              dtype=object)                                                                                                                        ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.iloc[[900],:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d68cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Optional\n",
    "\n",
    "def query_graphrag(\n",
    "    query: str,\n",
    "    method: str = \"global\",\n",
    "    root_path: str = \"./ragtest\",\n",
    "    timeout: Optional[int] = None,\n",
    "    community_level: int = 2,\n",
    "    dynamic_community_selection: bool = False\n",
    ") -> str:\n",
    "\n",
    "    if community_level < 0:\n",
    "        raise ValueError(\"Community level must be non-negative\")\n",
    "\n",
    "    command = [\n",
    "        \"graphrag\", \"query\",\n",
    "        \"--root\", root_path,\n",
    "        \"--method\", method,\n",
    "        \"--query\", query,\n",
    "        \"--community-level\", str(community_level),\n",
    "    ]\n",
    "\n",
    "    if dynamic_community_selection:\n",
    "        command.append(\"--dynamic-community-selection\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\",\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    # if GraphRAG fails, show stderr clearly\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            f\"GraphRAG failed (exit {result.returncode})\\n\\nSTDERR:\\n{result.stderr}\\n\\nSTDOUT:\\n{result.stdout}\"\n",
    "        )\n",
    "\n",
    "    return result.stdout.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b677cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is faced with the decision of selecting between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several factors must be considered. Each method has its unique advantages and challenges, and the choice largely depends on the specific needs of the project, available resources, and desired outcomes.\n",
      "\n",
      "### Understanding RAG and Its Benefits\n",
      "\n",
      "RAG combines retrieval and generation processes to enhance the quality of generated outputs by leveraging external information. This method is particularly beneficial when the goal is to improve accuracy and relevance in responses, as it allows models to access a broader range of data beyond their training set [Data: Reports (1); Entities (29)]. However, implementing RAG can introduce complexities related to data retrieval and integration, which must be carefully managed to ensure optimal performance [Data: Relationships (19)].\n",
      "\n",
      "### The Role of Fine-Tuning\n",
      "\n",
      "Fine-tuning is a critical process that adapts pre-trained models to specific tasks, significantly enhancing their performance. This method involves adjusting model parameters using task-specific datasets, which allows the model to better understand and respond to the nuances of the task at hand [Data: Reports (1); Entities (27)]. Companies may opt for full fine-tuning when they have sufficient computational resources and data, as this approach can lead to substantial performance improvements. However, it can also be resource-intensive and may not be feasible for all organizations.\n",
      "\n",
      "### Exploring Parameter-Efficient Fine-Tuning (PEFT)\n",
      "\n",
      "PEFT techniques, such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning (HFT), offer a more resource-efficient alternative to traditional fine-tuning. These methods focus on optimizing model performance while minimizing the number of parameters that need to be adjusted, thus conserving computational resources [Data: Entities (62, 11); Relationships (27)]. PEFT is particularly advantageous in scenarios where computational power is limited or when working with large models, as it allows for effective adaptation without extensive retraining [Data: Entities (287)].\n",
      "\n",
      "### Considerations for Choosing the Right Approach\n",
      "\n",
      "1. **Project Requirements**: Companies should evaluate the specific needs of their projects. If the goal is to enhance the model's ability to generate contextually relevant responses using external data, RAG may be the best choice. Conversely, if the focus is on adapting a model to a specific task with high accuracy, fine-tuning or PEFT might be more appropriate.\n",
      "\n",
      "2. **Resource Availability**: The availability of computational resources and data is a crucial factor. Full fine-tuning requires significant computational power and large datasets, while PEFT methods can be more manageable for organizations with limited resources [Data: Relationships (15)].\n",
      "\n",
      "3. **Performance Goals**: Companies must consider their performance goals. If achieving state-of-the-art results is essential, full fine-tuning may be necessary. However, if the goal is to maintain reasonable performance while optimizing resource usage, PEFT techniques could be the ideal solution [Data: Entities (62)].\n",
      "\n",
      "4. **Implementation Complexity**: The complexity of implementation should also be taken into account. RAG involves integrating retrieval mechanisms, which can complicate the deployment process. In contrast, PEFT methods are designed to be more straightforward, allowing for easier integration into existing workflows [Data: Relationships (15)].\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "In summary, the choice between RAG, fine-tuning, and PEFT approaches depends on a variety of factors, including project requirements, resource availability, performance goals, and implementation complexity. By carefully evaluating these aspects, companies can make informed decisions that align with their strategic objectives and operational capabilities. Each method has its strengths, and the right choice will ultimately depend on the specific context and needs of the organization.\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"local\",\n",
    "    root_path=r\"D:\\Narwal\\knowledge_graphs_using_networkX\\ragtest\",\n",
    "    community_level=2\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee396de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge_graphs_using_networkX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
